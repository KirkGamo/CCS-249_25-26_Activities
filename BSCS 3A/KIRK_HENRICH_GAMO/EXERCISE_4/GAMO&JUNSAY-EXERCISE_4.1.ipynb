{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d12fa2",
   "metadata": {},
   "source": [
    "# **Exercise for Unit 4.1 - NLP Text Classification**\n",
    "\n",
    "**Name:** Kirk Henrich Gamo & Myrrhea Belle B. Junsay <br>\n",
    "**Date:** February 12, 2026 <br>\n",
    "**Year and Section:** BSCS3A -AI <br>\n",
    "\n",
    "This notebook demonstrates the manual implementation of Naïve Bayes classifier for spam and ham classification using bag of words, priors, and likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9072adb",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "| doc | class |\n",
    "|-----|-------|\n",
    "| Free money now!!! | SPAM |\n",
    "| Hi mom, how are you? | HAM |\n",
    "| Lowest price for your meds | SPAM |\n",
    "| Are we still on for dinner? | HAM |\n",
    "| Win a free iPhone today | SPAM |\n",
    "| Let's catch up tomorrow at the office | HAM |\n",
    "| Meeting at 3 PM tomorrow | HAM |\n",
    "| Get 50% off, limited time! | SPAM |\n",
    "| Team meeting in the office | HAM |\n",
    "| Click here for prizes! | SPAM |\n",
    "| Can you send the report? | HAM |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcdc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "# Dataset\n",
    "documents = [\n",
    "    (\"Free money now!!!\", \"SPAM\"),\n",
    "    (\"Hi mom, how are you?\", \"HAM\"),\n",
    "    (\"Lowest price for your meds\", \"SPAM\"),\n",
    "    (\"Are we still on for dinner?\", \"HAM\"),\n",
    "    (\"Win a free iPhone today\", \"SPAM\"),\n",
    "    (\"Let's catch up tomorrow at the office\", \"HAM\"),\n",
    "    (\"Meeting at 3 PM tomorrow\", \"HAM\"),\n",
    "    (\"Get 50% off, limited time!\", \"SPAM\"),\n",
    "    (\"Team meeting in the office\", \"HAM\"),\n",
    "    (\"Click here for prizes!\", \"SPAM\"),\n",
    "    (\"Can you send the report?\", \"HAM\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5acde",
   "metadata": {},
   "source": [
    "## a. Generate a Bag of Words (for word frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf9d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART A: BAG OF WORDS\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 45\n",
      "\n",
      "Vocabulary: ['3', '50', 'a', 'are', 'at', 'can', 'catch', 'click', 'dinner', 'for', 'free', 'get', 'here', 'hi', 'how', 'in', 'iphone', 'let', 'limited', 'lowest', 'meds', 'meeting', 'mom', 'money', 'now', 'off', 'office', 'on', 'pm', 'price', 'prizes', 'report', 's', 'send', 'still', 'team', 'the', 'time', 'today', 'tomorrow', 'up', 'we', 'win', 'you', 'your']\n",
      "\n",
      "--- Word Frequency for SPAM ---\n",
      "{'free': 2, 'money': 1, 'now': 1, 'lowest': 1, 'price': 1, 'for': 2, 'your': 1, 'meds': 1, 'win': 1, 'a': 1, 'iphone': 1, 'today': 1, 'get': 1, '50': 1, 'off': 1, 'limited': 1, 'time': 1, 'click': 1, 'here': 1, 'prizes': 1}\n",
      "\n",
      "--- Word Frequency for HAM ---\n",
      "{'hi': 1, 'mom': 1, 'how': 1, 'are': 2, 'you': 2, 'we': 1, 'still': 1, 'on': 1, 'for': 1, 'dinner': 1, 'let': 1, 's': 1, 'catch': 1, 'up': 1, 'tomorrow': 2, 'at': 2, 'the': 3, 'office': 2, 'meeting': 2, '3': 1, 'pm': 1, 'team': 1, 'in': 1, 'can': 1, 'send': 1, 'report': 1}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: convert to lowercase and extract words\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    # Extract words (alphanumeric sequences)\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    return words\n",
    "\n",
    "def generate_bag_of_words(documents):\n",
    "    \"\"\"\n",
    "    Generate a bag of words with word frequency for each class\n",
    "    Returns:\n",
    "        - vocabulary: set of all unique words\n",
    "        - word_freq_by_class: dict with class -> word frequencies\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    word_freq_by_class = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for doc, label in documents:\n",
    "        words = preprocess_text(doc)\n",
    "        for word in words:\n",
    "            vocabulary.add(word)\n",
    "            word_freq_by_class[label][word] += 1\n",
    "    \n",
    "    return vocabulary, word_freq_by_class\n",
    "\n",
    "# Generate Bag of Words\n",
    "vocabulary, word_freq_by_class = generate_bag_of_words(documents)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART A: BAG OF WORDS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
    "print(f\"\\nVocabulary: {sorted(vocabulary)}\")\n",
    "\n",
    "print(f\"\\n--- Word Frequency for SPAM ---\")\n",
    "print(dict(word_freq_by_class['SPAM']))\n",
    "\n",
    "print(f\"\\n--- Word Frequency for HAM ---\")\n",
    "print(dict(word_freq_by_class['HAM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73cbf3",
   "metadata": {},
   "source": [
    "## b. Calculate the Prior for the classes HAM and SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b34263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PART B: CLASS PRIORS\n",
      "============================================================\n",
      "\n",
      "Total documents: 11\n",
      "\n",
      "Class counts:\n",
      "  SPAM: 5\n",
      "  HAM: 6\n",
      "\n",
      "Prior Probabilities:\n",
      "  P(SPAM) = 5/11 = 0.4545\n",
      "  P(HAM) = 6/11 = 0.5455\n"
     ]
    }
   ],
   "source": [
    "def calculate_priors(documents):\n",
    "    \"\"\"\n",
    "    Calculate prior probability P(class) for each class\n",
    "    Prior = (count of documents in class) / (total documents)\n",
    "    \"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    for doc, label in documents:\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    priors = {}\n",
    "    for class_label in class_counts:\n",
    "        priors[class_label] = class_counts[class_label] / total_docs\n",
    "    \n",
    "    return priors, class_counts\n",
    "\n",
    "priors, class_counts = calculate_priors(documents)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART B: CLASS PRIORS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal documents: {len(documents)}\")\n",
    "print(f\"\\nClass counts:\")\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f\"  {class_label}: {count}\")\n",
    "\n",
    "print(f\"\\nPrior Probabilities:\")\n",
    "for class_label, prior in priors.items():\n",
    "    print(f\"  P({class_label}) = {class_counts[class_label]}/{len(documents)} = {prior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2fd428",
   "metadata": {},
   "source": [
    "## c. Calculate the Likelihood of the tokens in the vocabulary with respect to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361f689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PART C: LIKELIHOODS P(word|class)\n",
      "============================================================\n",
      "\n",
      "Vocabulary size: 45\n",
      "\n",
      "--- Likelihood for SPAM (sample words) ---\n",
      "  P(free|SPAM) = 0.044776\n",
      "  P(for|SPAM) = 0.044776\n",
      "  P(money|SPAM) = 0.029851\n",
      "  P(now|SPAM) = 0.029851\n",
      "  P(lowest|SPAM) = 0.029851\n",
      "  P(price|SPAM) = 0.029851\n",
      "  P(your|SPAM) = 0.029851\n",
      "  P(meds|SPAM) = 0.029851\n",
      "  P(win|SPAM) = 0.029851\n",
      "  P(a|SPAM) = 0.029851\n",
      "\n",
      "--- Likelihood for HAM (sample words) ---\n",
      "  P(the|HAM) = 0.050633\n",
      "  P(are|HAM) = 0.037975\n",
      "  P(you|HAM) = 0.037975\n",
      "  P(tomorrow|HAM) = 0.037975\n",
      "  P(at|HAM) = 0.037975\n",
      "  P(office|HAM) = 0.037975\n",
      "  P(meeting|HAM) = 0.037975\n",
      "  P(hi|HAM) = 0.025316\n",
      "  P(mom|HAM) = 0.025316\n",
      "  P(how|HAM) = 0.025316\n"
     ]
    }
   ],
   "source": [
    "def calculate_likelihoods(vocabulary, word_freq_by_class, classes):\n",
    "    \"\"\"\n",
    "    Calculate likelihood P(word|class) for each word in vocabulary and each class\n",
    "    Using Laplace smoothing: P(word|class) = (count(word in class) + 1) / (total words in class + vocabulary size)\n",
    "    \"\"\"\n",
    "    likelihoods = {}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    for class_label in classes:\n",
    "        likelihoods[class_label] = {}\n",
    "        # Total word count in this class\n",
    "        total_words = sum(word_freq_by_class[class_label].values())\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            # Laplace smoothing: add 1 to count and to denominator\n",
    "            count = word_freq_by_class[class_label].get(word, 0)\n",
    "            likelihood = (count + 1) / (total_words + vocab_size)\n",
    "            likelihoods[class_label][word] = likelihood\n",
    "    \n",
    "    return likelihoods\n",
    "\n",
    "classes = ['HAM', 'SPAM']\n",
    "likelihoods = calculate_likelihoods(vocabulary, word_freq_by_class, classes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART C: LIKELIHOODS P(word|class)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
    "print(f\"\\n--- Likelihood for SPAM (sample words) ---\")\n",
    "spam_words = sorted(word_freq_by_class['SPAM'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for word, count in spam_words:\n",
    "    print(f\"  P({word}|SPAM) = {likelihoods['SPAM'][word]:.6f}\")\n",
    "\n",
    "print(f\"\\n--- Likelihood for HAM (sample words) ---\")\n",
    "ham_words = sorted(word_freq_by_class['HAM'].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for word, count in ham_words:\n",
    "    print(f\"  P({word}|HAM) = {likelihoods['HAM'][word]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19fe74",
   "metadata": {},
   "source": [
    "## d. Determine the class of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513cce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PART D: CLASSIFICATION OF TEST SENTENCES\n",
      "============================================================\n",
      "\n",
      "1. Test Sentence: \"Limited offer, click here!\"\n",
      "   Predicted Class: SPAM\n",
      "   Scores: HAM=-18.0839, SPAM=-15.5278\n",
      "\n",
      "2. Test Sentence: \"Meeting at 2 PM with the manager.\"\n",
      "   Predicted Class: HAM\n",
      "   Scores: HAM=-26.9156, SPAM=-30.2213\n"
     ]
    }
   ],
   "source": [
    "def predict_class(text, priors, likelihoods, vocabulary, classes):\n",
    "    \"\"\"\n",
    "    Predict the class of a given text using Naïve Bayes\n",
    "    P(class|document) ∝ P(class) * ∏P(word|class) for all words in document\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    \n",
    "    scores = {}\n",
    "    for class_label in classes:\n",
    "        # Start with the prior probability (using log for numerical stability)\n",
    "        score = math.log(priors[class_label])\n",
    "        \n",
    "        # Multiply likelihoods of all words in the document\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                score += math.log(likelihoods[class_label][word])\n",
    "            else:\n",
    "                # For unknown words, use smoothed likelihood\n",
    "                score += math.log(1 / (sum(word_freq_by_class[class_label].values()) + len(vocabulary)))\n",
    "        \n",
    "        scores[class_label] = score\n",
    "    \n",
    "    # Find the class with the highest score\n",
    "    predicted_class = max(scores, key=scores.get)\n",
    "    return predicted_class, scores\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART D: CLASSIFICATION OF TEST SENTENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Limited offer, click here!\",\n",
    "    \"Meeting at 2 PM with the manager.\"\n",
    "]\n",
    "\n",
    "for i, test_sentence in enumerate(test_sentences, 1):\n",
    "    predicted_class, scores = predict_class(test_sentence, priors, likelihoods, vocabulary, classes)\n",
    "    print(f\"\\n{i}. Test Sentence: \\\"{test_sentence}\\\"\")\n",
    "    print(f\"   Predicted Class: {predicted_class}\")\n",
    "    print(f\"   Scores: HAM={scores['HAM']:.4f}, SPAM={scores['SPAM']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f8bf1",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MANUAL NAÏVE BAYES - SUMMARY\n",
      "============================================================\n",
      "\n",
      "✓ Bag of Words Generated: Vocabulary size = 45\n",
      "\n",
      "✓ Class Priors Calculated:\n",
      "  - P(HAM) = 0.5455\n",
      "  - P(SPAM) = 0.4545\n",
      "\n",
      "✓ Likelihoods Calculated: P(word|class) for all 45 words\n",
      "\n",
      "✓ Test Sentences Classified:\n",
      "  1. \"Limited offer, click here!\" → SPAM\n",
      "  2. \"Meeting at 2 PM with the manager.\" → HAM\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MANUAL NAÏVE BAYES - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✓ Bag of Words Generated: Vocabulary size = {len(vocabulary)}\")\n",
    "print(f\"\\n✓ Class Priors Calculated:\")\n",
    "for class_label in classes:\n",
    "    print(f\"  - P({class_label}) = {priors[class_label]:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Likelihoods Calculated: P(word|class) for all {len(vocabulary)} words\")\n",
    "\n",
    "print(f\"\\n✓ Test Sentences Classified:\")\n",
    "for i, test_sentence in enumerate(test_sentences, 1):\n",
    "    predicted_class, _ = predict_class(test_sentence, priors, likelihoods, vocabulary, classes)\n",
    "    print(f\"  {i}. \\\"{test_sentence}\\\" → {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411185",
   "metadata": {},
   "source": [
    "## Part 2: Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8156b5f",
   "metadata": {},
   "source": [
    "Use the scikit-learn package to train and test a Multinomial Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17623d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free money now!!!</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi mom, how are you?</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lowest price for your meds</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are we still on for dinner?</td>\n",
       "      <td>HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Win a free iPhone today</td>\n",
       "      <td>SPAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           doc class\n",
       "0            Free money now!!!  SPAM\n",
       "1         Hi mom, how are you?   HAM\n",
       "2   Lowest price for your meds  SPAM\n",
       "3  Are we still on for dinner?   HAM\n",
       "4      Win a free iPhone today  SPAM"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Convert the existing documents dataset to a DataFrame\n",
    "docs = [doc for doc, label in documents]\n",
    "labels = [label for doc, label in documents]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'doc': docs,\n",
    "    'class': labels\n",
    "})\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499ab0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes model trained successfully using a pipeline.\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(df['doc'], df['class'])\n",
    "\n",
    "print(\"Multinomial Naive Bayes model trained successfully using a pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d89eb6",
   "metadata": {},
   "source": [
    "### a. Determine the class of the following test sentences:\n",
    "1. Limited offer, click here!\n",
    "2. Meeting at 2 PM with the manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc8ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predictions using Scikit-Learn Multinomial Naïve Bayes ---\n",
      "\n",
      "'Limited offer, click here!' is classified as: SPAM\n",
      "'Meeting at 2 PM with the manager.' is classified as: HAM\n",
      "\n",
      "--- Prediction process complete ---\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'Limited offer, click here!',\n",
    "    'Meeting at 2 PM with the manager.'\n",
    "]\n",
    "\n",
    "print(\"\\n--- Predictions using Scikit-Learn Multinomial Naïve Bayes ---\\n\")\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    prediction = model.predict([sentence])\n",
    "    print(f\"'{sentence}' is classified as: {prediction[0]}\")\n",
    "\n",
    "print(\"\\n--- Prediction process complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
